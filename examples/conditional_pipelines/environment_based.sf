-- Environment-based Pipeline Example
-- This example demonstrates using conditionals to adapt pipeline behavior
-- based on the environment (production, staging, development)

-- Define data sources with different connection details for each environment
SOURCE postgres_source (
    connector_type = "POSTGRES",
    host = "${db_host|localhost}",
    port = "${db_port|5432}",
    database = "${db_name|app_db}",
    username = "${db_user|postgres}",
    password = "${db_password}",
    schema = "${db_schema|public}"
);

SOURCE s3_source (
    connector_type = "S3",
    bucket = "${data_bucket|example-data}",
    region = "${aws_region|us-east-1}",
    access_key = "${aws_access_key}",
    secret_key = "${aws_secret_key}"
);

-- Use different data sources based on environment
IF ${env} == 'production' THEN
    -- In production, load directly from the production database
    LOAD users FROM SOURCE postgres_source
    WITH (
        query = "SELECT * FROM users WHERE status = 'active'"
    );
ELSE IF ${env} == 'staging' THEN
    -- In staging, load from a staging database with fewer records
    LOAD users FROM SOURCE postgres_source
    WITH (
        query = "SELECT * FROM users WHERE status = 'active' LIMIT 1000"
    );
ELSE
    -- In development, load from a local CSV file via S3
    LOAD users FROM SOURCE s3_source
    WITH (
        path = "sample_data/users.csv",
        format = "csv",
        header = "true"
    );
END IF;

-- Perform different transformations based on environment
IF ${env} == 'production' THEN
    -- Apply stricter filtering in production
    CREATE TABLE filtered_users AS
    SELECT 
        id,
        email,
        first_name,
        last_name,
        created_at,
        updated_at
    FROM users
    WHERE 
        email IS NOT NULL 
        AND verified = true
        AND created_at > '2020-01-01';
ELSE
    -- Apply looser filtering in non-production environments
    CREATE TABLE filtered_users AS
    SELECT 
        id,
        email,
        first_name,
        last_name,
        created_at,
        updated_at
    FROM users
    WHERE email IS NOT NULL;
END IF;

-- Create reports with different levels of aggregation
CREATE TABLE user_stats AS
SELECT
    COUNT(*) as total_users,
    COUNT(DISTINCT email_domain) as domain_count,
    MIN(created_at) as oldest_account,
    MAX(created_at) as newest_account
FROM (
    SELECT
        id,
        SUBSTRING(email FROM POSITION('@' IN email) + 1) as email_domain,
        created_at
    FROM filtered_users
);

-- Use different export destinations based on environment
IF ${env} == 'production' THEN
    -- Export to production data warehouse in production
    EXPORT TO bigquery_destination
    FROM user_stats
    WITH (
        project = "my-production-project",
        dataset = "analytics",
        table = "user_statistics",
        create_disposition = "CREATE_IF_NEEDED",
        write_disposition = "WRITE_TRUNCATE"
    );
ELSE IF ${env} == 'staging' THEN
    -- Export to staging data warehouse in staging
    EXPORT TO bigquery_destination
    FROM user_stats
    WITH (
        project = "my-staging-project",
        dataset = "analytics",
        table = "user_statistics",
        create_disposition = "CREATE_IF_NEEDED",
        write_disposition = "WRITE_TRUNCATE"
    );
ELSE
    -- Export to local file in development
    EXPORT TO file_destination
    FROM user_stats
    WITH (
        path = "./output/user_stats.json",
        format = "json"
    );
END IF;

-- Always export filtered users to S3 (regardless of environment)
-- but with environment-specific paths
EXPORT TO s3_destination
FROM filtered_users
WITH (
    bucket = "${export_bucket|my-data-exports}",
    path = "${env|dev}/users/filtered_users_${date}.csv",
    format = "csv",
    header = "true"
); 