-- =================================================================
-- SHOWCASE MULTI-CONNECTOR: Demonstrates SQLFlow's connector ecosystem
-- =================================================================
-- Run with: docker compose exec sqlflow sqlflow pipeline run /app/sqlflow/demos/ecommerce_demo/pipelines/showcase_02_multi_connector_integration.sf --vars '{"date": "2023-10-25", "API_TOKEN": "demo-token"}'

-- Define dynamic variables
SET date = "${date|2023-10-25}";
SET api_token = "${API_TOKEN|demo-token}";

-- =================================================================
-- PART 1: MULTIPLE DATA SOURCES
-- =================================================================

-- ----------------------------------------
-- PostgreSQL Data Source
-- ----------------------------------------
-- Connect to PostgreSQL database
SOURCE pg_sales TYPE POSTGRES PARAMS {
  "host": "postgres",
  "port": 5432,
  "dbname": "ecommerce",
  "user": "sqlflow",
  "password": "sqlflow123",
  "table": "sales"
};

SOURCE pg_customers TYPE POSTGRES PARAMS {
  "host": "postgres",
  "port": 5432,
  "dbname": "ecommerce",
  "user": "sqlflow",
  "password": "sqlflow123",
  "table": "customers"
};

SOURCE pg_products TYPE POSTGRES PARAMS {
  "host": "postgres",
  "port": 5432,
  "dbname": "ecommerce",
  "user": "sqlflow",
  "password": "sqlflow123",
  "table": "products"
};

-- ----------------------------------------
-- CSV Data Source (for comparison/backup)
-- ----------------------------------------
SOURCE csv_sales TYPE CSV PARAMS {
  "path": "data/sales.csv",
  "has_header": true
};

-- =================================================================
-- PART 2: DATA LOADING & TRANSFORMATION
-- =================================================================

-- Load data from each source
LOAD sales_from_postgres FROM pg_sales;
LOAD customers FROM pg_customers;
LOAD products FROM pg_products;

-- Load CSV data separately
LOAD sales_from_csv FROM csv_sales;

-- ----------------------------------------
-- Data Validation & Comparison
-- ----------------------------------------
-- Compare PostgreSQL data with CSV data
CREATE TABLE source_comparison AS 
SELECT
  'PostgreSQL' AS source,
  COUNT(*) AS row_count 
FROM sales_from_postgres
UNION ALL
SELECT
  'CSV' AS source,
  COUNT(*) AS row_count 
FROM sales_from_csv;

-- ----------------------------------------
-- Create enriched data for analysis
-- ----------------------------------------
CREATE TABLE sales_enriched AS 
SELECT
  s.order_id,
  s.customer_id,
  c.name AS customer_name,
  c.email,
  c.region,
  s.product_id,
  p.name AS product_name,
  p.category,
  s.quantity,
  s.price,
  s.quantity * s.price AS total_amount,
  s.order_date
FROM sales_from_postgres s
JOIN customers c ON s.customer_id = c.customer_id
JOIN products p ON s.product_id = p.product_id;

-- =================================================================
-- PART 3: ANALYTICS
-- =================================================================

-- Customer segmentation by region and order value
CREATE TABLE customer_segments AS
SELECT
  region,
  CASE 
    WHEN total_spent >= 1000 THEN 'Premium'
    WHEN total_spent >= 500 THEN 'Standard'
    ELSE 'Basic'
  END AS customer_segment,
  COUNT(DISTINCT customer_id) AS num_customers,
  AVG(total_spent) AS avg_spent_per_customer
FROM (
  SELECT 
    customer_id,
    region,
    SUM(total_amount) AS total_spent
  FROM sales_enriched
  GROUP BY customer_id, region
) t
GROUP BY region, customer_segment
ORDER BY region, avg_spent_per_customer DESC;

-- Product performance metrics
CREATE TABLE product_performance AS
SELECT
  product_id,
  product_name,
  category,
  COUNT(DISTINCT order_id) AS orders,
  SUM(quantity) AS units_sold,
  SUM(total_amount) AS revenue,
  SUM(total_amount) / SUM(quantity) AS avg_price
FROM sales_enriched
GROUP BY product_id, product_name, category
ORDER BY revenue DESC;

-- =================================================================
-- PART 4: MULTI-CONNECTOR EXPORTS
-- =================================================================

-- ----------------------------------------
-- Export 1: To S3 (MinIO)
-- ----------------------------------------
-- Export analytics data to S3 in Parquet format
EXPORT SELECT * FROM product_performance
TO "s3://analytics/reports/product_performance_${date}.parquet"
TYPE S3
OPTIONS { 
  "format": "parquet",
  "compression": "snappy",
  "endpoint_url": "http://minio:9000",
  "region": "us-east-1",
  "access_key": "minioadmin",
  "secret_key": "minioadmin",
  "bucket": "analytics"
};

EXPORT SELECT * FROM customer_segments
TO "s3://analytics/reports/customer_segments_${date}.parquet"
TYPE S3
OPTIONS { 
  "format": "parquet",
  "compression": "snappy",
  "endpoint_url": "http://minio:9000",
  "region": "us-east-1",
  "access_key": "minioadmin",
  "secret_key": "minioadmin",
  "bucket": "analytics"
};

-- ----------------------------------------
-- Export 2: To PostgreSQL
-- ----------------------------------------
-- Write results back to PostgreSQL
EXPORT SELECT * FROM customer_segments
TO "postgres://sqlflow:sqlflow123@postgres:5432/ecommerce/customer_segments_${date}"
TYPE POSTGRES
OPTIONS {
  "if_exists": "replace"
};

-- ----------------------------------------
-- Export 3: To REST API
-- ----------------------------------------
-- Send a notification with summary metrics to a REST API
EXPORT SELECT 
  '${date}' AS report_date,
  (SELECT COUNT(DISTINCT customer_id) FROM sales_enriched) AS total_customers,
  (SELECT COUNT(DISTINCT order_id) FROM sales_enriched) AS total_orders,
  (SELECT SUM(total_amount) FROM sales_enriched) AS total_revenue,
  (SELECT COUNT(*) FROM customer_segments WHERE customer_segment = 'Premium') AS premium_customers
TO "http://mockserver:1080/api/reports/notification"
TYPE REST
OPTIONS {
  "method": "POST",
  "headers": {
    "Content-Type": "application/json",
    "Authorization": "Bearer ${api_token}"
  }
};

-- ----------------------------------------
-- Export 4: To CSV
-- ----------------------------------------
-- Also export to CSV for easy viewing
EXPORT SELECT * FROM source_comparison
TO "target/source_comparison_${date}.csv"
TYPE CSV
OPTIONS { 
  "header": true
}; 