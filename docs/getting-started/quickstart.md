# Tutorial: SQLFlow Quickstart - From Zero to Working Pipeline

> **Function**: This tutorial gets you from zero to a working SQLFlow data pipeline with real results in under 1 minute. No prior SQLFlow knowledge required.

**Difficulty Level**: üü¢ Beginner  
**Time to Complete**: Under 1 minute  
**Verification Status**: ‚úÖ Verified against SQLFlow v0.1.7+

## Prerequisites

### Required Knowledge
- Basic command line usage
- No SQL or data pipeline experience required

### Required Setup
- Python 3.10 or higher
- `pip` package manager
- Terminal/command prompt access

### Before You Start
```bash
# Verify prerequisites
python --version  # Should show 3.10+
pip --version     # Should show pip is available
```

## Learning Objectives

By the end of this tutorial, you will be able to:
- Install SQLFlow and verify it works
- Create a SQLFlow project with sample data
- Run your first data pipeline and see real results
- Understand the basic SQLFlow project structure

## Overview

This tutorial demonstrates SQLFlow's **industry-leading time to value**: from installation to working analytics in under 60 seconds. You'll analyze realistic customer data and generate business insights immediately.

### What You'll Build
A complete customer analytics pipeline that:
- Loads 1,000 customers, 5,000 orders, and 500 products
- Generates customer segmentation analysis by country and tier
- Identifies top customers by spending
- Exports results to CSV files for further analysis

### Example Data
Auto-generated realistic datasets:
- **Customers**: 1,000 records with demographics and tier information
- **Orders**: 5,000 transactions with products and pricing
- **Products**: 500 catalog items with categories
**Source**: Auto-generated by `sqlflow init` command

---

## Step 1: Install SQLFlow

**Time**: 15 seconds  
**Objective**: Get SQLFlow installed and verify it works

SQLFlow installs as a single Python package with everything needed for data analytics:

### Code
```bash
# Install SQLFlow with all core analytics capabilities
pip install sqlflow-core
```

### Verification
```bash
# Verify installation worked
sqlflow --version
```

### Expected Output
```
SQLFlow version: 0.1.7
```

---

## Step 2: Create Your First Project

**Time**: 15 seconds  
**Objective**: Create a working project with sample data and pipelines

SQLFlow's `init` command creates everything you need instantly:

### Code
```bash
# Create project with realistic sample data and working pipelines
sqlflow init my-first-project

# Navigate to your new project
cd my-first-project
```

### Key Concepts Introduced
- **Project Structure**: Organized directories for data, pipelines, and configuration
- **Sample Data Generation**: Automatic creation of realistic test datasets
- **Ready-to-Run Pipelines**: Pre-built analytics examples

### Expected Output
```
üöÄ Creating sample data and working pipelines...
‚úÖ Project 'my-first-project' initialized successfully!
üìä Created 1,000 customers, 5,000 orders, and 500 products
üîß Ready-to-run pipelines: example, customer_analytics, data_quality

Next steps:
  cd my-first-project
  sqlflow pipeline run customer_analytics  # Immediate results!
```

### Verification
```bash
# Check project structure was created
ls -la
```

**Expected**: Directories for `data/`, `pipelines/`, `profiles/`, and `README.md`

---

## Step 3: Run Your First Pipeline

**Time**: 15 seconds  
**Objective**: Execute a complete data pipeline and generate business insights

Now run the customer analytics pipeline to see SQLFlow in action:

### Code
```bash
# Run customer analytics pipeline (works immediately!)
sqlflow pipeline run customer_analytics
```

### Key Concepts Introduced
- **Pipeline Execution**: Automatic dependency resolution and execution
- **DuckDB Engine**: Lightning-fast in-memory analytics
- **Data Export**: Automatic CSV generation from SQL results

### Expected Output
```
[SQLFlow] Using profile: dev
üö® Running in DuckDB memory mode: results will NOT be saved after process exit.
Running pipeline: ./pipelines/customer_analytics.sf
üìù Compiling customer_analytics.sf
‚è±Ô∏è  Starting execution at 17:43:23
üîÑ Creating customers
üîÑ Creating orders  
üîÑ Creating products
üîÑ Creating customer_summary
üîÑ Creating top_customers
üì§ Exported customer_summary.csv (32 rows)
üì§ Exported top_customers.csv (20 rows)
‚è±Ô∏è  Execution completed in 0.08 seconds
‚úÖ Pipeline completed successfully
```

### Verification
```bash
# Check outputs were created
ls output/
```

**Expected**: `customer_summary.csv` and `top_customers.csv` files

---

## Step 4: View Your Results

**Time**: 15 seconds  
**Objective**: Examine the business insights generated by your pipeline

### Code
```bash
# Check customer segmentation analysis
head -5 output/customer_summary.csv
```

### Expected Output
```csv
country,tier,customer_count,avg_age,total_orders,total_revenue
Italy,bronze,289,43.89,289,92908.34
UK,bronze,299,42.87,299,90810.47
Australia,bronze,281,43.83,279,80078.78
Germany,bronze,270,42.21,270,79112.98
```

```bash
# View top customers by spending
head -5 output/top_customers.csv
```

### Expected Output
```csv
name,email,tier,country,order_count,total_spent
Matthew Martin,matthew.martin@example.com,bronze,UK,10,5189.92
Lisa Jackson,lisa.jackson@example.com,gold,US,14,4867.66
Emma Moore,emma.moore@example.com,platinum,Australia,11,4815.13
Ryan Jackson,ryan.jackson@example.com,silver,US,12,4630.71
```

### Verification
```bash
# Count rows to verify data quality
wc -l output/*.csv
```

**Expected**: ~32 rows in customer_summary.csv, ~20 rows in top_customers.csv

---

## Final Verification

### Complete Example
Here's the complete pipeline you just ran:

```sql
-- Complete customer analytics pipeline
-- Source: pipelines/customer_analytics.sf

-- Load data using DuckDB's read_csv_auto function
CREATE TABLE customers AS
SELECT * FROM read_csv_auto('data/customers.csv');

CREATE TABLE orders AS
SELECT * FROM read_csv_auto('data/orders.csv');

CREATE TABLE products AS
SELECT * FROM read_csv_auto('data/products.csv');

-- Create customer summary by country and tier
CREATE TABLE customer_summary AS
SELECT 
    c.country,
    c.tier,
    COUNT(*) as customer_count,
    AVG(c.age) as avg_age,
    COUNT(o.order_id) as total_orders,
    COALESCE(SUM(o.price * o.quantity), 0) as total_revenue
FROM customers c
LEFT JOIN orders o ON c.customer_id = o.customer_id
GROUP BY c.country, c.tier
ORDER BY total_revenue DESC;

-- Find top customers by spending  
CREATE TABLE top_customers AS
SELECT 
    c.name,
    c.email,
    c.tier,
    c.country,
    COUNT(o.order_id) as order_count,
    SUM(o.price * o.quantity) as total_spent
FROM customers c
JOIN orders o ON c.customer_id = o.customer_id
GROUP BY c.customer_id, c.name, c.email, c.tier, c.country
ORDER BY total_spent DESC
LIMIT 20;

-- Export results
EXPORT SELECT * FROM customer_summary TO "output/customer_summary.csv" TYPE CSV OPTIONS { "header": true };
EXPORT SELECT * FROM top_customers TO "output/top_customers.csv" TYPE CSV OPTIONS { "header": true };
```

### Test Your Understanding
Try these variations to ensure you understand the concepts:

1. **Run other pipelines**:
   ```bash
   sqlflow pipeline run data_quality
   cat output/data_quality_report.csv
   ```

2. **Explore the sample data**:
   ```bash
   head data/customers.csv
   head data/orders.csv
   ```

### Expected Results
After completing all steps, you should have:
- Working SQLFlow installation
- Project with realistic sample data (1,000 customers, 5,000 orders, 500 products)
- Generated customer analytics insights in CSV format
- Understanding of SQLFlow's basic workflow

---

## Troubleshooting

### Common Issues

#### Issue: "No module named sqlflow"
**When it happens**: During Step 1 (installation verification)
**Symptoms**: 
```
ModuleNotFoundError: No module named 'sqlflow'
```
**Solution**:
1. Ensure you're in the correct virtual environment
2. Reinstall SQLFlow:
   ```bash
   pip install sqlflow-core
   python -c "import sqlflow; print('‚úÖ SQLFlow installed')"
   ```

#### Issue: "Directory already exists"
**When it happens**: During Step 2 (project creation)
**Symptoms**: Error when running `sqlflow init my-first-project`
**Solution**: Choose a different project name or remove existing directory:
```bash
sqlflow init my-project-$(date +%s)  # Creates unique name
```

#### Issue: Pipeline execution fails
**When it happens**: During Step 3 (pipeline execution)
**Symptoms**: Pipeline fails with SQL or file errors
**Solution**:
1. Verify you're in the project directory: `ls` should show `pipelines/`, `data/`, etc.
2. Check pipeline syntax: `sqlflow pipeline validate customer_analytics`
3. Re-initialize if needed: `cd .. && rm -rf my-first-project && sqlflow init my-first-project`

### Getting Help
If you encounter issues not covered here:
1. Check the [Installation Guide](installation.md) for environment-specific issues
2. Review the [complete troubleshooting guide](../user-guide/troubleshooting/)
3. Visit our [Community Forum](https://github.com/sqlflow/sqlflow/discussions)

---

## Next Steps

### Immediate Next Steps
- **[Installation Guide](installation.md)**: Learn about advanced installation options and environment setup
- **[User Guide Fundamentals](../user-guide/fundamentals/)**: Understand core SQLFlow concepts and architecture

### Related Documentation
- **[Pipeline Development](../user-guide/pipelines/)**: Learn to create your own custom pipelines
- **[Data Connectors](../user-guide/connectors/)**: Connect to databases and cloud storage
- **[CLI Reference](../reference/cli.md)**: Complete command reference

### Practice Exercises
1. **Exercise 1**: Modify the customer_analytics pipeline to add product analysis
   - **Goal**: Add insights about product performance by category
   - **Hint**: Join the products table and group by product category

2. **Exercise 2**: Create your own simple pipeline
   - **Goal**: Write a new `.sf` file that analyzes order patterns by date
   - **Solution**: Check `/examples/` directory for similar patterns

---

## Summary

### What You Learned
- **SQLFlow Installation**: Single command installation with verification
- **Project Structure**: Auto-generated projects with data, pipelines, and configuration
- **Pipeline Execution**: Running SQL-based data pipelines with dependency resolution
- **Data Export**: Automatic CSV generation from SQL query results

### Key Takeaways
- **Speed**: SQLFlow delivers results in under 1 minute from installation
- **Simplicity**: Standard SQL with intelligent extensions, no complex frameworks
- **Completeness**: Full data pipeline from ingestion to export in single tool
- **Developer Experience**: Instant feedback with realistic sample data

### Complete Code Reference
All code from this tutorial is available at:
- **Tutorial Project**: Created by `sqlflow init my-first-project`
- **Pipeline Code**: `pipelines/customer_analytics.sf` in your project
- **Sample Data**: `data/*.csv` files in your project

üéâ **Congratulations!** You've completed the SQLFlow quickstart and can now build data pipelines faster than any other tool in the industry.

**üìö Continue Learning**: [Installation Guide](installation.md) | [User Guide Fundamentals](../user-guide/fundamentals/) | [More Examples](../examples/)