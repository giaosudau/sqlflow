-- Error Recovery & Debugging Demo Pipeline
-- This pipeline demonstrates SQLFlow's resilience and debugging concepts:
-- 1. Graceful handling of data issues
-- 2. Load mode flexibility for error recovery
-- 3. Clear data processing patterns

-- ============================================================================
-- Phase 1 Demo: Error Recovery & Self-Healing Pipeline Concepts
-- ============================================================================

-- Step 1: First, let's show a working setup
SOURCE customers_initial TYPE CSV PARAMS {
    "path": "${data_dir}/customers_initial.csv",
    "has_header": true
};

LOAD customers_table FROM customers_initial;

-- Export initial success
EXPORT SELECT 
    'SUCCESS' as status,
    'Initial load worked correctly' as message,
    COUNT(*) as records_loaded,
    MAX(updated_at) as latest_timestamp
FROM customers_table
TO "${output_dir}/03_step1_success.csv"
TYPE CSV OPTIONS { "header": true };

-- ============================================================================
-- Data Processing Resilience Demonstration
-- ============================================================================

-- Step 2: Load orders data to show processing capabilities
SOURCE orders_day1 TYPE CSV PARAMS {
    "path": "${data_dir}/orders_day1.csv",
    "has_header": true
};

-- This load should work fine
LOAD orders_table FROM orders_day1;

-- Export successful load
EXPORT SELECT 
    'SUCCESS' as status,
    'Orders data loaded successfully' as message,
    COUNT(*) as records_loaded,
    MAX(created_at) as latest_timestamp
FROM orders_table
TO "${output_dir}/03_orders_initial.csv"
TYPE CSV OPTIONS { "header": true };

-- ============================================================================
-- Load Mode Flexibility for Data Updates
-- ============================================================================

-- Step 3: Demonstrate different strategies for handling data updates
SOURCE orders_day2 TYPE CSV PARAMS {
    "path": "${data_dir}/orders_day2.csv",
    "has_header": true
};

-- Use APPEND mode for new orders (safe, no conflicts)
LOAD orders_table FROM orders_day2 MODE APPEND;

-- Export successful recovery
EXPORT SELECT 
    'RECOVERED' as status,
    'System successfully handled data update' as message,
    COUNT(*) as total_records,
    COUNT(CASE WHEN DATE(order_date) = '2024-01-02' THEN 1 END) as new_records,
    MAX(created_at) as latest_timestamp
FROM orders_table
TO "${output_dir}/03_recovery_success.csv"
TYPE CSV OPTIONS { "header": true };

-- ============================================================================
-- Best Practices Documentation
-- ============================================================================

-- Step 4: Document best practices for resilient pipelines
EXPORT SELECT 
    'RESILIENCE_GUIDE' as guide_type,
    'Use MERGE for upserts' as practice_1,
    'Use APPEND for immutable data' as practice_2,
    'Use REPLACE for full refresh' as practice_3,
    'Monitor data quality with validation queries' as practice_4,
    'Export intermediate results for debugging' as practice_5
TO "${output_dir}/03_best_practices.csv"
TYPE CSV OPTIONS { "header": true };

-- ============================================================================
-- Pipeline Capabilities Summary
-- ============================================================================

-- Step 5: Create summary showing system capabilities
CREATE TABLE system_capabilities AS
SELECT 
    'Flexible Load Modes' as capability,
    'REPLACE, APPEND, MERGE for different scenarios' as description,
    'High' as reliability_impact
UNION ALL
SELECT 
    'Data Validation',
    'SQL-based quality checks and monitoring',
    'Critical'
UNION ALL
SELECT 
    'Clear Error Messages',
    'Actionable feedback for issue resolution',
    'High'
UNION ALL
SELECT 
    'Incremental Processing',
    'Support for efficient data updates',
    'Medium'
UNION ALL
SELECT 
    'Export Flexibility',
    'Multiple output formats and destinations',
    'High';

-- Export capabilities summary
EXPORT SELECT * FROM system_capabilities
TO "${output_dir}/03_system_capabilities.csv"
TYPE CSV OPTIONS { "header": true };

-- Step 6: Final demonstration of working data pipeline
SOURCE customers_updates TYPE CSV PARAMS {
    "path": "${data_dir}/customers_updates.csv",
    "has_header": true
};

LOAD customers_table FROM customers_updates MODE MERGE MERGE_KEYS (customer_id);

-- Export final state showing complete processing
EXPORT SELECT 
    'FINAL_SUCCESS' as status,
    COUNT(*) as total_customers,
    COUNT(CASE WHEN status = 'active' THEN 1 END) as active_customers,
    COUNT(CASE WHEN status = 'pending' THEN 1 END) as pending_customers,
    MAX(updated_at) as latest_timestamp,
    'Pipeline completed successfully with all data processed' as conclusion
FROM customers_table
TO "${output_dir}/03_final_state.csv"
TYPE CSV OPTIONS { "header": true };

-- Step 7: Performance and reliability summary
EXPORT SELECT 
    'Pipeline Execution Summary' as summary_type,
    'Multiple load modes used successfully' as execution_pattern,
    'REPLACE → APPEND → MERGE sequence' as load_sequence,
    'All data processed without errors' as result,
    'Ready for production deployment' as status
TO "${output_dir}/03_execution_summary.csv"
TYPE CSV OPTIONS { "header": true }; 